{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/aditisaini/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/aditisaini/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer \n",
    "from nltk.tokenize.punkt import PunktSentenceTokenizer, PunktParameters\n",
    "from nltk.tag import pos_tag\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from matplotlib import pyplot as plt\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "import heapdict\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def performStemming(tokens):\n",
    "    stemmed_words = []\n",
    "    ps = PorterStemmer() \n",
    "    for token in tokens:\n",
    "        stemmed = ps.stem(token)\n",
    "        stemmed_words.append(stemmed)\n",
    "    return stemmed_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeStopWords(tokens):\n",
    "    all_stopwords = stopwords.words('english')\n",
    "    tokens_without_sw = [word for word in tokens if not word in all_stopwords]\n",
    "    return tokens_without_sw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def performSentenceSegmentation(file_content):\n",
    "    #Training the model using given text: unsupervised learning\n",
    "    tokenizer = PunktSentenceTokenizer()\n",
    "    tokenizer.train(file_content)\n",
    "    sentence_segmentation = tokenizer.tokenize(file_content)\n",
    "    return sentence_segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def performPOSTagging(sentences):\n",
    "    pos_tagged = {}\n",
    "    for sentence in sentences:\n",
    "        pos_tagged[sentence] = pos_tag(word_tokenize(sentence))\n",
    "    return pos_tagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get sentence length\n",
    "def averageSentenceLength(segmented_sentence):\n",
    "    total = 0\n",
    "    size = len(segmented_sentence)\n",
    "    for s in segmented_sentence:\n",
    "        words = s.split()\n",
    "        total+=len(words)\n",
    "    return total/size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting graph\n",
    "def plotgraph(freqdict, graphname, xlabel, ylabel):\n",
    "    x = list(freqdict.keys())\n",
    "    y = list(freqdict.values())\n",
    "    plt.figure()\n",
    "    plt.bar(x, y, width=1.0)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.tight_layout()\n",
    "    #plt.show()\n",
    "    plt.savefig(graphname + '.png')\n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the x-axis is the length of a token in number of characters, and the y-axis is the number of tokens of each length\n",
    "def visualTokenAnalysis(tokens):\n",
    "    token_analysis = {}\n",
    "    for token in tokens:\n",
    "        if len(token) in token_analysis.keys():\n",
    "            token_analysis[len(token)]+=1\n",
    "        else:\n",
    "            token_analysis[len(token)]=1\n",
    "    return token_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the x-axis is the length of a sentence in number of tokens/words, and the y-axis is the number of sentences of each length\n",
    "def visualSentenceAnalysis(sentence_segmentation):\n",
    "    sentence_analysis = {}\n",
    "    for sentence in sentence_segmentation:\n",
    "        words = word_tokenize(sentence)\n",
    "        words_size = len(words)\n",
    "        if words_size in sentence_analysis.keys():\n",
    "            sentence_analysis[words_size]+=1\n",
    "        else:\n",
    "            sentence_analysis[words_size]=1\n",
    "    return sentence_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top20Words(tokens):\n",
    "    token_size = heapdict.heapdict() \n",
    "    for token in tokens:\n",
    "        if token in token_size.keys():\n",
    "            token_size[token]-=1\n",
    "        else:\n",
    "            token_size[token]=-1\n",
    "    top20 = []\n",
    "    for poptokens in range(20):\n",
    "        top20.append(token_size.popitem()[0])\n",
    "    return top20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def performVisualAnalysis(token_analysis, stemmed_token_analysis, sentence_analysis):\n",
    "    plotgraph(token_analysis, \"Token length analysis\", \"length of a token in number of characters\", \"number of tokens of each length\")\n",
    "    plotgraph(sentence_analysis, \"Sentence length analysis\", \"length of a sentence in number of words/tokens\", \"number of sentences each length\")\n",
    "    plotgraph(stemmed_token_analysis, \"Stemmed token length analysis\", \"length of a stemmed token in number of characters\", \"number of stemmed tokens of each length\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_phrases(my_tree, phrase):\n",
    "    my_phrases = []\n",
    "    if my_tree.label() == phrase:\n",
    "        my_phrases.append(my_tree.copy(True))\n",
    "    for child in my_tree:\n",
    "        if type(child) is nltk.Tree:\n",
    "            list_of_phrases = extract_phrases(child, phrase)\n",
    "            if len(list_of_phrases) > 0:\n",
    "                my_phrases.extend(list_of_phrases)\n",
    "\n",
    "    return my_phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def improveTokeniser(sentence_segmentation):\n",
    "    phrases = []\n",
    "    grammar = \"NP: {<JJ>*<NN>|<NNP>*}\"\n",
    "    cp = nltk.RegexpParser(grammar)\n",
    "    for x in sentence_segmentation:\n",
    "        sentence = pos_tag(word_tokenize(x))\n",
    "        tree = cp.parse(sentence)\n",
    "        list_of_noun_phrases = extract_phrases(tree, 'NP')\n",
    "        for phrase in list_of_noun_phrases:\n",
    "            phrases.append(\"_\".join([x[0] for x in phrase.leaves()]))\n",
    "    return phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printPOSTagged(pos_tagged):\n",
    "    for tag in pos_tag:\n",
    "        print(tag)\n",
    "        print(\"\\n\")\n",
    "        print(pos_tag[tag])\n",
    "        print('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    #1: Load databases\n",
    "    file_content = open('datasets/dataset1.txt').read()\n",
    "    #2: Split the content for each review\n",
    "    each_reviews = file_content.split(\"---xxx---\")\n",
    "    #3: Tokenisation\n",
    "    #tokens = word_tokenize(file_content)\n",
    "    tokens = text_to_word_sequence(file_content)\n",
    "    #4: Remove stop words from text\n",
    "    tokens_without_sw = removeStopWords(tokens)\n",
    "    #5: Stemming\n",
    "    stemmed_words = performStemming(tokens_without_sw)\n",
    "    #6: Top 20 words\n",
    "    top20 = top20Words(tokens_without_sw)\n",
    "    top20StemmedWords = top20Words(stemmed_words)\n",
    "    #7: Sentence segmentation\n",
    "    sentence_segmentation = performSentenceSegmentation(file_content)\n",
    "    #8: Improving tokeniser by extracting noun phrases\n",
    "    phrases = improveTokeniser(sentence_segmentation)\n",
    "    #9: POS Tagging\n",
    "    ##9.1 Sentences from each datasets 1, 2, 3\n",
    "    sentences = [\"All restaurants have children’s menus.\", \"\", \"\"]\n",
    "    ##9.2 POS Tagged sentences\n",
    "    pos_tagged = performPOSTagging(sentences)\n",
    "    #10: Average length of each sentence\n",
    "    avg_length = averageSentenceLength(sentence_segmentation)\n",
    "    #11: Graphical analysis\n",
    "    token_analysis = visualTokenAnalysis(tokens_without_sw)\n",
    "    stemmed_token_analysis = visualTokenAnalysis(stemmed_words)\n",
    "    sentence_analysis = visualSentenceAnalysis(sentence_segmentation)\n",
    "    performVisualAnalysis(token_analysis, stemmed_token_analysis, sentence_analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'function' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-952aa76662a6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"All restaurants have children’s menus.\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Complimentary amenities include a welcome pack and daily ice-cream passes\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"You won’t have to jostle with other hotel guests even if there’s a crowd.\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mpos_tagged\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mperformPOSTagging\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mprintPOSTagged\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos_tagged\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-23-d035fabecbab>\u001b[0m in \u001b[0;36mprintPOSTagged\u001b[0;34m(pos_tagged)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mprintPOSTagged\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos_tagged\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mtag\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpos_tag\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos_tag\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'function' object is not iterable"
     ]
    }
   ],
   "source": [
    "file_content = open('datasets/dataset1.txt').read()\n",
    "sentences = [\"All restaurants have children’s menus.\", \"Complimentary amenities include a welcome pack and daily ice-cream passes\", \"You won’t have to jostle with other hotel guests even if there’s a crowd.\"]\n",
    "pos_tagged = performPOSTagging(sentences)\n",
    "printPOSTagged(pos_tagged)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ureca]",
   "language": "python",
   "name": "conda-env-ureca-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
