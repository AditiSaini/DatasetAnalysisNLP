{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/aditisaini/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/aditisaini/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer \n",
    "from nltk.tokenize.punkt import PunktSentenceTokenizer, PunktParameters\n",
    "from nltk.tag import pos_tag\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from matplotlib import pyplot as plt\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "import heapdict\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def performStemming(tokens):\n",
    "    stemmed_words = []\n",
    "    ps = PorterStemmer() \n",
    "    for token in tokens:\n",
    "        stemmed = ps.stem(token)\n",
    "        stemmed_words.append(stemmed)\n",
    "    return stemmed_words\n",
    "\n",
    "\n",
    "def removeStopWords(tokens):\n",
    "    all_stopwords = stopwords.words('english')\n",
    "    tokens_without_sw = [word for word in tokens if not word in all_stopwords]\n",
    "    return tokens_without_sw\n",
    "\n",
    "\n",
    "def performSentenceSegmentation(file_content):\n",
    "    #Training the model using given text: unsupervised learning\n",
    "    tokenizer = PunktSentenceTokenizer()\n",
    "    tokenizer.train(file_content)\n",
    "    sentence_segmentation = tokenizer.tokenize(file_content)\n",
    "    return sentence_segmentation\n",
    "\n",
    "\n",
    "def performPOSTagging(sentences):\n",
    "    pos_tagged = {}\n",
    "    for sentence in sentences:\n",
    "        pos_tagged[sentence] = pos_tag(word_tokenize(sentence))\n",
    "    return pos_tagged\n",
    "\n",
    "def printPOSTagged(pos_tagged):\n",
    "    for tag in pos_tagged:\n",
    "        print(tag)\n",
    "        print(pos_tagged[tag])\n",
    "        print('\\n\\n')\n",
    "\n",
    "#Get sentence length\n",
    "def averageSentenceLength(segmented_sentence):\n",
    "    total = 0\n",
    "    size = len(segmented_sentence)\n",
    "    for s in segmented_sentence:\n",
    "        words = s.split()\n",
    "        total+=len(words)\n",
    "    return total/size\n",
    "\n",
    "\n",
    "#Plotting graph\n",
    "def plotgraph(freqdict, graphname, xlabel, ylabel):\n",
    "    x = list(freqdict.keys())\n",
    "    y = list(freqdict.values())\n",
    "    plt.figure()\n",
    "    plt.bar(x, y, width=1.0)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.tight_layout()\n",
    "    #plt.show()\n",
    "    plt.savefig(graphname + '.png')\n",
    "    plt.clf()\n",
    "\n",
    "\n",
    "#the x-axis is the length of a token in number of characters, and the y-axis is the number of tokens of each length\n",
    "def visualTokenAnalysis(tokens):\n",
    "    token_analysis = {}\n",
    "    for token in tokens:\n",
    "        if len(token) in token_analysis.keys():\n",
    "            token_analysis[len(token)]+=1\n",
    "        else:\n",
    "            token_analysis[len(token)]=1\n",
    "    return token_analysis\n",
    "\n",
    "\n",
    "#the x-axis is the length of a sentence in number of tokens/words, and the y-axis is the number of sentences of each length\n",
    "def visualSentenceAnalysis(sentence_segmentation):\n",
    "    sentence_analysis = {}\n",
    "    for sentence in sentence_segmentation:\n",
    "        words = word_tokenize(sentence)\n",
    "        words_size = len(words)\n",
    "        if words_size in sentence_analysis.keys():\n",
    "            sentence_analysis[words_size]+=1\n",
    "        else:\n",
    "            sentence_analysis[words_size]=1\n",
    "    return sentence_analysis\n",
    "\n",
    "\n",
    "def top20Words(tokens):\n",
    "    token_size = heapdict.heapdict() \n",
    "    for token in tokens:\n",
    "        if token in token_size.keys():\n",
    "            token_size[token]-=1\n",
    "        else:\n",
    "            token_size[token]=-1\n",
    "    top20 = []\n",
    "    for poptokens in range(20):\n",
    "        top20.append(token_size.popitem()[0])\n",
    "    return top20\n",
    "\n",
    "\n",
    "def performVisualAnalysis(dataset, token_analysis, stemmed_token_analysis, sentence_analysis):\n",
    "    plotgraph(token_analysis, \"Token length analysis_\" + dataset, \"length of a token in number of characters\", \"number of tokens of each length\")\n",
    "    plotgraph(sentence_analysis, \"Sentence length analysis_\" + dataset, \"length of a sentence in number of words/tokens\", \"number of sentences each length\")\n",
    "    plotgraph(stemmed_token_analysis, \"Stemmed token length analysis_\" + dataset, \"length of a stemmed token in number of characters\", \"number of stemmed tokens of each length\")\n",
    "\n",
    "\n",
    "def extract_phrases(my_tree, phrase):\n",
    "    my_phrases = []\n",
    "    if my_tree.label() == phrase:\n",
    "        my_phrases.append(my_tree.copy(True))\n",
    "    for child in my_tree:\n",
    "        if type(child) is nltk.Tree:\n",
    "            list_of_phrases = extract_phrases(child, phrase)\n",
    "            if len(list_of_phrases) > 0:\n",
    "                my_phrases.extend(list_of_phrases)\n",
    "\n",
    "    return my_phrases\n",
    "\n",
    "\n",
    "def improveTokeniser(sentence_segmentation):\n",
    "    phrases = []\n",
    "    grammar = \"NP: {<JJ>*<NN>|<NNP>*}\"\n",
    "    cp = nltk.RegexpParser(grammar)\n",
    "    for x in sentence_segmentation:\n",
    "        sentence = pos_tag(word_tokenize(x))\n",
    "        tree = cp.parse(sentence)\n",
    "        list_of_noun_phrases = extract_phrases(tree, 'NP')\n",
    "        for phrase in list_of_noun_phrases:\n",
    "            phrases.append(\"_\".join([x[0] for x in phrase.leaves()]))\n",
    "    return phrases\n",
    "\n",
    "def writeToFile(filename, value):\n",
    "    text_file = open(filename, \"w\")\n",
    "    text_file.write(value)\n",
    "    text_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(dataset):\n",
    "    print(\"\\n\\n\\n\\n\\n\")\n",
    "    print(\".........STARTING ANALYSIS.........\")\n",
    "    print(\"\\n\\n\")\n",
    "    #1: Load databases\n",
    "    file_content = open('datasets/' + dataset).read()\n",
    "    #2: Split the content for each review\n",
    "    each_reviews = file_content.split(\"---xxx---\")\n",
    "    #3: Tokenisation\n",
    "    tokens = text_to_word_sequence(file_content)\n",
    "    writeToFile(\"1_tokens_\" + dataset, \"\\n\".join(tokens))\n",
    "    #4: Remove stop words from text\n",
    "    tokens_without_sw = removeStopWords(tokens)\n",
    "    writeToFile(\"2_tokens_without_sw_\" + dataset, \"\\n\".join(tokens_without_sw))\n",
    "    #5: Stemming\n",
    "    stemmed_words = performStemming(tokens_without_sw)\n",
    "    writeToFile(\"3_stemmed_words_\" + dataset, \"\\n\".join(stemmed_words))\n",
    "    #6: Top 20 words\n",
    "    top20 = top20Words(tokens_without_sw)\n",
    "    print(\"Top 20 words: \")\n",
    "    print(top20)\n",
    "    print(\"\\n\\n--------------\\n\\n\")\n",
    "    top20StemmedWords = top20Words(stemmed_words)\n",
    "    print(\"Top 20 stemmed words: \")\n",
    "    print(top20StemmedWords)\n",
    "    print(\"\\n\\n--------------\\n\\n\")\n",
    "    #7: Sentence segmentation\n",
    "    sentence_segmentation = performSentenceSegmentation(file_content)\n",
    "    writeToFile(\"4_sentence_segmentation_\" + dataset, \"\\n\".join(sentence_segmentation))\n",
    "    #8: Improving tokeniser by extracting noun phrases\n",
    "    phrases = improveTokeniser(sentence_segmentation)\n",
    "    writeToFile(\"5_phrases_with_improved_tokeniser_\" + dataset, \"\\n\".join(phrases))\n",
    "    #9: POS Tagging\n",
    "    ##9.1 Sentences from each datasets 1, 2, 3\n",
    "    if dataset==\"dataset1.txt\":\n",
    "        sentences = [\"All restaurants have children’s menus.\", \"Complimentary amenities include a welcome pack and daily ice-cream passes\", \"You won’t have to jostle with other hotel guests even if there’s a crowd.\"]\n",
    "    elif dataset==\"dataset2.txt\":\n",
    "        sentences = [\"Now your work is saved on the branch 'my-saved-work' in case you decide you want it back (or want to look at it later or diff it against your updated branch).\", \n",
    "        \"Note that the first example assumes that the remote repo's name is 'origin' and that the branch named 'master' in the remote repo matches the currently checked-out branch in your local repo.\", \n",
    "        \"BTW, this situation that you're in looks an awful lot like a common case where a push has been done into the currently checked out branch of a non-bare repository.\"]\n",
    "    elif dataset==\"dataset3.txt\":\n",
    "        sentences = [\"Unfortunately, no consensus has  emerged  about  the  form  or  the  existence  of  such  a  data  structure.\", \n",
    "        \"Such systems are often viewed as software components for constructing real-world NLP solutions.\", \n",
    "        \"Interactive Voice Response (IVR) applications used in call centers to respond to certain users’ requests.\"]\n",
    "    ##9.2 POS Tagged sentences\n",
    "    pos_tagged = performPOSTagging(sentences)\n",
    "    print(\"POS Tagged sentence: \")\n",
    "    printPOSTagged(pos_tagged)\n",
    "    print(\"\\n\\n--------------\\n\\n\")\n",
    "    #10: Average length of each sentence\n",
    "    avg_length = averageSentenceLength(sentence_segmentation)\n",
    "    print(\"Avg length of a sentence: \")\n",
    "    print(avg_length)\n",
    "    print(\"\\n\\n--------------\\n\\n\")\n",
    "    #11: Graphical analysis\n",
    "    token_analysis = visualTokenAnalysis(tokens_without_sw)\n",
    "    stemmed_token_analysis = visualTokenAnalysis(stemmed_words)\n",
    "    sentence_analysis = visualSentenceAnalysis(sentence_segmentation)\n",
    "    performVisualAnalysis(dataset, token_analysis, stemmed_token_analysis, sentence_analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      ".........STARTING ANALYSIS.........\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'datasets/-f'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-d94c264e78c2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-29-0dd28d5ff95a>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(dataset)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m#1: Load databases\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mfile_content\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'datasets/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;31m#2: Split the content for each review\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0meach_reviews\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile_content\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"---xxx---\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'datasets/-f'"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    dataset = sys.argv[1]\n",
    "    main(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ureca]",
   "language": "python",
   "name": "conda-env-ureca-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
